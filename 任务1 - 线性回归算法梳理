【任务1 - 线性回归算法梳理】时长：2天
机器学习的一些概念
有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
线性回归的原理
线性回归损失函数、代价函数、目标函数
优化方法(梯度下降法、牛顿法、拟牛顿法等)
线性回归的评估指标
sklearn参数详解


有监督学习，无监督学习
有监督学习：根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。也就是说，在监督学习中训练数据既有特征(feature)
又有标签(label)，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标记
无监督学习：对没有概念标记(分类）的训练样本进行学习，以发现训练样本集中的结构性知识。在这里，所有的标记是未知的。因此，训练的样本的歧意极高，聚类就是典
型的无监督学习


什么时候使用有监督学习，什么时候使用无监督学习





泛化能力
泛化能力是模型对未知数据的预测能力。大白话来说就是，模型训好了，放到实际场景中去使用，会不会掉链子，还是能达到跟训练时一样的效果。
泛化能力的本质就是反映模型有没有对客观世界做真实的刻画，还是发生了过拟合。


过拟合欠拟合(方差和偏差以及各自解决办法)
出现过拟合得原因：
1）建模样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或
业务场景； 
（2）样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系； 
（3）建模时的“逻辑假设”到了模型应用时已经不能成立了。任何预测模型都是在假设的基础上才可以搭建和应用的，常用的假设包括：假设历史数据可以推测未来，假设
业务环节没有发生显著变化，假设建模数据与后来的应用数据是相似的，等等。如果上述假设违反了业务场景的话，根据这些假设搭建的模型当然是无法有效应用的。 
（4）参数太多、模型复杂度高 
（5）决策树模型。如果我们对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据(event)或非事件数据（no event）
，可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。 
解决方法：
（1）权值衰减. 主要应用在神经网络模型中 
它在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的 
惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏。 
（2）适当的stopping criterio
在二次误差函数的情况下，关于早停止和权值衰减类似结果的原因说明。椭圆给出了常数误差函数的轮廓线，Wml表示误差函数的最小值。如果权向量的起始点为原点，按照
局部负梯度的方向移动，那么它会沿着曲线给出的路径移动。通过对训练过程早停止，我们找到了一个权值向量w。定性地说，它类似于使用检点的权值衰减正则化项，然后
最小化正则化误差函数的方法得到的权值。 
（3）验证数据 
一个最成功的方法是在训练数据外再为算法提供一套验证数据,应该使用在验证集合上产生最小误差 的迭代次数,不是总能明显地确定验证集合何时达到最小误差. 


欠拟合得原因：
首先欠拟合就是模型没有很好地捕捉到数据特征，不能够很好地拟合数据

解决欠拟合得方法：
1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。

2）添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。例如上面的图片的例子。

3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。



交叉验证
第一种是简单交叉验证，所谓的简单，是和其他交叉验证方法相对而言的。首先，我们随机的将样本数据分为两部分（比如： 70%的训练集，30%的测试集），然后用训练
集来训练模型，在测试集上验证模型及参数。接着，我们再把样本打乱，重新选择训练集和测试集，继续训练数据和检验模型。最后我们选择损失函数评估最优的模型和参
数。　

第二种是S折交叉验证（S-Folder Cross Validation）。和第一种方法不同，S折交叉验证会把样本数据随机的分成S份，每次随机的选择S-1份作为训练集，剩下的1份
做测试集。当这一轮完成后，重新随机选择S-1份来训练数据。若干轮（小于S）之后，选择损失函数评估最优的模型和参数。

第三种是留一交叉验证（Leave-one-out Cross Validation），它是第二种情况的特例，此时S等于样本数N，这样对于N个样本，每次选择N-1个样本来训练数据，留一
个样本来验证模型预测的好坏。此方法主要用于样本量非常少的情况，比如对于普通适中问题，N小于50时，我一般采用留一交叉验证。


线性回归的原理
定义：线性回归是利用线性回归方程对一个或多个自变量和因变量之间的关系进行建模的一种回归分析方法，只有一个自变量的情况称为一元线性回归，大于一个自变量情况的叫
做多元线性回归。


线性回归方程的
1. 什么是代价函数？













































